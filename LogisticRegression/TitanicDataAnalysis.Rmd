---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

Hi MLEnthusiasts! Today, we will learn how to implement logistic regression using R that too on a well-known dataset, The Titanic Dataset! So, our analysis becomes by getting some information about the dataset, like what all variables are in our dataset and what do we have to predict. 

The dataset can be found on this link of kaggle. Following are the variables of this dataset:
survival: Tells whether a particular passenger survived or not. 0 for not survived, 1 for survived.
pClass: Ticket class, 1 for 1st class, 2 for 2nd class and 3 for 3rd class.
sex: Tells us the gender of the passenger
Age: in years
sibsp: # of siblings or spouses aboarding the titanic
parch: # of parents/children of the passenger
fare: passenger fare
embarked: The port of embarkment; C for Cherbourg, Q for Queenstown and S for Southampton

Having seen what the data is all about, let's also understand the problem statement. The problem is to make a logistic regression model using all the variables mentioned above with dependent variable being Survived and other variables being independent. Thus, it will be a predictive model which predicts whether a passenger having given parameters will survive or not. By looking closely at the problem, we can say that it's a binary classification problem(0/1) or logistic regression problem. 

Let us first set our working directory and import our dataset. 

```{r}
setwd("C:/Users/jyoti/Downloads/LogisticRegression")
data <- read.csv("titanic.csv")
```

Here, data is a dataframe having all the variables and data of those variables. The dataframe has 1257 observations of 9 variables. 
The next step is to view the data inside the dataframe.
```{r}
View(data)
```
Now starts the first main step, "Data Preparation". To see if there is any missing data or to know about the mean or standard deviation, we use the summary() function.
```{r}
summary(data)
```
As can be seen, there are 261 missing values in the Age variable. We need to do missing value imputation in this case. But, before doing that, we need to check how the age distribution looks like so that we can know which imputation method to choose and apply. 
```{r}
hist(data$age)
```
Since the distribution looks somewhat normal, we can use mean value imputation in this case. That is, we can replace the missing values with the mean of the age. This doesn't deviate the mean and the distribution of the age remains the same. 
```{r}
data$age[is.na(data$age)] = 29.07
summary(data)
```
As can be seen above, age doesn't have any missing value now.
Let's see how the data looks like now.
```{r}
head(data)
```
Now, let us understand the concept of dummy variables. Suppose a variable "A" has n classes. This variable A can be replaced by n-1 variables. If A has i, j, k, ..., classes, then A_i = 1 in the rows at which i appears in A's column and 0 for the rest of the rows. Same applies for j, k.. etc. The last value gets taken care of by the intercept.
So, let's introduce dummy variables inside our data for sex and embarked columns since they are holding the categorical data.
```{r}
data$female = ifelse(data$sex=="female", 1, 0)
data$embarked_c = ifelse(data$embarked=="C", 1, 0)
data$embarked_s = ifelse(data$embarked=="S", 1, 0)
head(data)
```
Now, if you have a look at dataframe, it contains 12 variables and not 9.
The next step is to remove those variables which we no longer need, name, sex since it is already taken into account by female variable, embarked, i.e. column number 3, 4 and 9.
```{r}
data = data[-c(3, 4, 9)]
head(data)
```
Let's do univariate analysis of the numerical variables, age and fare now. 
```{r}
bx = boxplot(data$age)
```
Thus, there are outliers in the age variable and we need to do outlier handling in this case.
```{r}
bx$stats
quantile(data$age, seq(0, 1, 0.02))
```
We can replace the outliers above 96% of the quantile range and below 4% of the quantile range so that more accuracy is obtained and the data loss is also not very significant.
```{r}
data$age = ifelse(data$age>=52, 52, data$age)
data$age = ifelse(data$age<=4, 4, data$age)
boxplot(data$age)
```
The boxplot comes out to be neat in this case after outlier handling. Let us now do analysis for fare variable.
```{r}
bx = boxplot(data$fare)
```
```{r}
bx$stats
```
Thus, there is a very large amount of outlier data on the upper end.
```{r}
quantile(data$fare, seq(0, 1, 0.02))
```
As can be seen above, the major difference between the values arises above 96% of the quantile. 
```{r}
data$fare = ifelse(data$fare>=136, 136, data$fare)
boxplot(data$fare)
```
Let us now start our bivariate analysis. 
```{r}
library(car)
scatterplot(data$age, data$survived)
```
It is to be noted that children and old passengers were saved first during the titanic mishap.
```{r}
scatterplot(data$fare, data$survived)
```
Let's now divide the data into train and test sets.
```{r}
set.seed(222)
t= sample(1:nrow(data), 0.7*nrow(data))
train = data[t,]
test = data[-t,]
```
Here, we have taken the train to test ratio as 7:3. Let's now make a model and check for multi-collinearity using variance inflation factor technique.
```{r}
library(car)
model <- lm(survived~., data=train)
t = vif(model)
sort(t, decreasing=TRUE)
```
As you can see, all the values of VIF for all the variables are less than 5, we need not reject any varible and we can straight away start our analysis.
```{r}
model1<- glm(as.factor(survived)~., family="binomial", data=train)
summary(model1)
```
As you can see, for some variables like parch, embarked_c and embarked_s, the P value is greater than 0.05. Thus, here we cannot reject null hypothesis that there is no relation between survived and them. Thus, we need to accept the null hypothesis and discard these three variables from our analysis.
Well, step function does it all for us.
```{r}
stepmodel = step(model1, direction="both")
formula(stepmodel)
summary(stepmodel)
```
Thus, now the main formula becomes as.factor(survived) ~ pclass + age + sibsp + female + embarked_c.
Now, we can use stepmodel to predict the score for train dataset.
```{r}
train$score <- predict(stepmodel, newdata = train, type="response")
head(train$score)
tail(train$score)
```
These are the probabilities values of whether the corresponding passenger survived or not.
Let's now start the model evaluation.
```{r}
library(lattice)
library(ggplot2)
library(caret)
library(e1071)
train$prediction <- ifelse(train$score>=0.6, 1, 0)
table(factor(train$prediction), factor(train$survived))
```
Thus, accuracy = (TP+TN)/(TP+TN+FP+FN)=(508+197)/(508+197+42+132)=705/879=0.802=80.2%. 
Now, let's check the ROC and AUC curves of the model.
```{r}
library(InformationValue)
plotROC(actuals=train$survived, predictedScores=as.numeric(fitted(stepmodel)))
```
```{r}
ks_plot(actuals=train$survived, predictedScores=as.numeric(fitted(stepmodel)))
```
Thus, the model has AUCRC value equal to 0.8456 which implies that the model quality is very good.
Now predict the scores on the test data.
```{r}
test$score<-predict(stepmodel, test, type = "response")
head(test$score)
```
Let's set the threshold to 0.6.
```{r}
test$predicted<-ifelse(test$score>=0.6, 1, 0)
head(test$predicted)
```
Let's now have a look at the confusion matrix for test dataset. 
```{r}
table(factor(test$predicted), factor(test$survived))
```
Thus, accuracy = 298/378 = 78.83%. Thus the model gives 80.02% accuracy on train dataset and 78.83% accuracy on test dataset.